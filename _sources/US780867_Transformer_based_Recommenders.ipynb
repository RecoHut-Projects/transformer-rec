{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmTFtNQFXExo"
      },
      "source": [
        "# Transformer-based Recommenders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa9a52297728"
      },
      "source": [
        "Early works on sequential recommendation usually capture sequential patterns from user historical interactions using Markov chains (MCs). For example, Shani et al. formalized recommendation generation as a sequential optimization problem and employ Markov Decision Processes (MDPs) to address it. Later, Rendle et al. combine the power of MCs and MF to model both sequential behaviors and general interests by Factorizing Personalized Markov Chains (FPMC). Besides the first-order MCs, high-order MCs are also adopted to consider more previous items.\n",
        "\n",
        "Recently, RNN and its variants, Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), are becoming more and more popular for modeling user behavior sequences. The basic idea of these methods is to encode user’s previous records into a vector (i.e., representation of user’s preference which is used to make predictions) with various recurrent architectures and loss functions, including session-based GRU with ranking loss (GRU4Rec), Dynamic Recurrent Basket Model (DREAM), user-based GRU, attention-based GRU (NARM), and improved GRU4Rec with new loss function (i.e., BPR-max and TOP1-max) and an improved sampling strategy.\n",
        "\n",
        "Sequential dynamics are a key feature of many modern recommender systems, which seek to capture the ‘context’ of users’ activities on the basis of actions they have performed recently. To capture such patterns, these two approaches have proliferated. Markov Chains assume that a user’s next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable.\n",
        "\n",
        "Other than recurrent neural networks, various deep learning models are also introduced for sequential recommendation. For example, Tang and Wang propose a Convolutional Sequence Model (Caser) to learn sequential patterns using both horizontal and vertical convolutional filters. Chen et al. and Huang et al. employ Memory Network to improve sequential recommendation. STAMP captures both users’ general interests and current interests using an MLP network with attention.\n",
        "\n",
        "Sequential models (like RNNs) try to encode the user preferences into a single vector using left-to-right unidirectional sequence. The major limitation is that such unidirectional models restrict the power of hidden representation for items in the historical sequences, where each item can only encode the information from previous items. Another limitation is that previous unidirectional models are originally introduced for sequential data with natural order, e.g., text and time series data. They often assume a rigidly ordered sequence over data which is not always true for user behaviors in real-world applications. In fact, the choices of items in a user’s historical interactions may not follow a rigid order assumption due to various unobservable external factors. In such a situation, it is crucial to incorporate context from both directions in user behavior sequence modeling.\n",
        "\n",
        "Both session-based and sequential (i.e., next-basket) recommendation algorithms take advantage of additional temporal information to make better personalized recommendations. The main difference between session-based recommendations and sequential recommendations is that the former assumes that the user ids are not recorded and therefore the length of engagement sequences are relatively short. Therefore, session-based recommendations normally do not consider user factors. On the other hand, sequential recommendation treats each sequence as a user’s engagement history. Both settings, do not explicitly require time-stamps: only the relative temporal orderings are assumed known (in contrast to, for example, timeSVD++ using time-stamps).\n",
        "\n",
        "The task of session-based recommendation is to predict the next item that users would click from the current clicks sequence. Many kinds of proposals for session-based recommendation methods have been developed due to the highly practical value. Markov chain is a classic approach which assumes that the next action depends on the last or last few behaviors. Only considering a few of last behaviors makes the Markov chain based models unable to utilize the dependencies of behaviors in long sequences and also might suffer from data sparsity issues. Rendle et al. optimizes a pairwise ranking objective function via stochastic gradient descent, which ignores the temporal dependence among interactions and only considers some low-order interactions of latent factors. In recent years, many researches apply Recurrent Neural Networks (RNNs) for session-based recommendation and achieved much better performance than conventional methods. Hidasi et al. apply RNNs with ranking loss for session-based recommendation. Li et al. propose NARM which models the user’s sequential behavior and main purpose simultaneously. Liu et al. propose STAMP which is capable of capturing users’ general interests from the long-term memory of a session context. Ren et al. take repeat consumption phenomenon into account and introduce repeat explore mechanism, which significantly improve the performance under repeat session scenario. Wang et al. bring collaborative modeling into this field with an end-to-end model. However, the RNNs based methods is hard to learn the dependencies from long distance. When the session sequence becomes longer, the performance of RNNs based methods decrease significantly. Recently, Graph neural networks (GNNs) have revolutionized the field of session-based recommendation which was dominated by recurrent neural networks (RNNs) by considering the transitions of items. Wu et al. proposed SR-GNN which models session sequences as graph-structured data and capture complex transitions of items. Yu et al. propose a target attentive network which discover the relevance of target item with graph neural network. Chen et al. tackle information loss of GNN-based model by introducing shortcut graph attention and edge-order preserving aggregation layers. However, the GNNs based methods only learn the dependencies among adjacent items of the session graphs which is constructed from session sequences. When the items nodes are not adjacent in the session graph, their dependencies are hard to capture with GNNs even if their dependencies cannot be ignored. \n",
        "\n",
        "Although the methods above achieved promising results and become the state-of-the-arts, the recurrent neural networks are hard to learn item dependencies from long distance. The graph neural networks based methods only aggregate the information from adjacent items where items are closely connected in the session graph. An attention based model have merge in the field of natural language processing, named the Transformer proposed by Vaswani et al.. It achieved excellent performance in the WMT 2014 English-to-French and English-to-German translation tasks. The key factor of its success is self-attention mechanism which allow the model capture the dependencies between encoder and decoder. However, the encoder-decoder architecture of the Transformer is not suit for session-based recommendation which output size is not equal to input size. Xu et al. apply self-attention layers after graph neural network to learn long-range dependencies among the output of graph neural network. However, the dependencies among all items may lost during the aggregation of adjacent items with the graph neural network. Adjacent items in a session graph may not be close in latent space, and vice versa. This may lead to inaccurate item embedding learning. The self-attention layer is directly applied after embedding layer to reserve the global item dependencies in SR-SAN.\n",
        "\n",
        "To deal with sequential and session-based recommendation, many sequence learning algorithms previously applied in machine learning and NLP research have been explored for RecSys, based on k-Nearest Neighbors, Frequent Pattern Mining, Hidden Markov Models, Recurrent Neural Networks, and more recently neural architectures using the Self-Attention Mechanism and the Transformer architectures.\n",
        "\n",
        "Much of the recent progress in sequential and session-based recommendation has been driven by improvements in model architecture and pre-training techniques originating in the field of Natural Language Processing. Transformer architectures in particular have facilitated building higher-capacity models and provided data augmentation and training techniques which demonstrably improve the effectiveness of sequential recommendation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "514a1114b4a2"
      },
      "source": [
        "## Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7a1c7f17772"
      },
      "source": [
        "## Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4089a0bfac4e"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e404a2eed171"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd32a5472e33"
      },
      "source": [
        "## Use cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e26abddc144"
      },
      "source": [
        "## Tutorials\n",
        "\n",
        "[](T034923_BERT4Rec_on_ML1M_in_PyTorch)\n",
        "[](T595874_BERT4Rec_on_ML25M_in_PyTorch_Lightning)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "US780867_Transformer_based_Recommenders.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
