{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5ab8d410-2dca-45b4-b9c1-8056d1277c05",
      "metadata": {
        "id": "6977c2d25fb8"
      },
      "source": [
        "# GeLU activation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a4a9ad",
      "metadata": {
        "id": "97301c94aca1"
      },
      "source": [
        "The Gaussian Error Linear Unit (GELU) activation function was introduced in 2018 by UC Berkeley’s Dan Hendrycks and Kevin Gimpel from the Toyota Technological Institute at Chicago. An activation function is the “switch” that triggers neuron output, and its importance has grown as networks have deepened. In recent weeks a number of discussions in the machine learning community have brought GELU back into the spotlight.\n",
        "\n",
        "Early artificial neurons utilized binary threshold units. These hard binary decisions were smoothed with sigmoid activations, enabling a neuron to have a “firing rate” interpretation and to train with backpropagation. This made ReLU (Rectified Linear Units) the most popular activation function due to its feature of gating decisions based upon an input’s sign.\n",
        "\n",
        "Hendrycks and Gimpel proposed the non-linear activation function GELU, a formulation that relates to stochastic regularizers because it is a modified expectation of adaptive dropout, providing neuron output a higher probabilistic view.\n",
        "\n",
        "We can expand the cumulative distribution of N(0,1), i.e. Φ(x), as follows:\n",
        "\n",
        "$$\\text{GELU}(x):=x{\\Bbb P}(X \\le x)=x\\Phi(x)=0.5x\\left(1+\\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$$\n",
        "\n",
        "Note that this is a *definition*, not an equation (or a relation). Authors have provided some justifications for this proposal, e.g. a stochastic *analogy*, however mathematically, this is just a definition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5053f329",
      "metadata": {
        "id": "c544544b0703"
      },
      "source": [
        "<p><center><img src='_images/US780867_7.png'></p></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c88da43",
      "metadata": {
        "id": "3821bf213868"
      },
      "source": [
        "Compared to ReLU or leaky ReLU, GELU has the theoretical advantage of being differentiable for all values of x, but has the in-practice disadvantage of being much, much more complex to compute.\n",
        "\n",
        "One can approximate the GELU with $0.5x\\left(1+\\tanh\\left[\\sqrt{2/\\pi}\\left(x + 0.044715x^{3}\\right)\\right]\\right)$ or $x\\sigma\\left(1.702x\\right)$, but PyTorch's exact implementation is sufficiently fast such that these approximations may be unnecessary."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "L201302_GeLU_activation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
