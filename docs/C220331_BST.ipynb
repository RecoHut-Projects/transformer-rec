{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C220331 | BST: Behavior Sequence Transformer for E-commerce Recommendation in Alibaba",
      "provenance": [],
      "authorship_tag": "ABX9TyOzX6bu9kzNgqMrPIYYv75C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms2vT6Po0Zm2"
      },
      "source": [
        "# BST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pAv64hF0tbV"
      },
      "source": [
        "Deep learning based methods have been widely used in industrial recommendation systems (RSs). Previous works adopt an Embedding & MLP paradigm: raw features are embedded into low-dimensional vectors, which are then fed onto MLP for final recommendations. However, most of these works just concatenate different features, ignoring the sequential nature of users' behaviors. BST model uses the powerful Transformer model to capture the sequential signals underlying users' behavior sequences for recommendation.\n",
        "\n",
        "Inspired by the great success of the Transformer for machine translation task in natural language processing (NLP) , this model applies the self-attention mechanism to learn a better representation for each item in a user’s behavior sequence by considering the sequential information in embedding stage, and then feed them into MLPs to predict users’ responses to candidate items. The key advantage of the Transformer is that it can better capture the dependency among words in sentences by the self-attention mechanism, and intuitively speaking, the “dependency” among items in users’ behavior sequences can also be extracted by the Transformer.\n",
        "\n",
        "<p><center><img src='_images/C220331_1.png'></p></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7YsaPP-0iXU"
      },
      "source": [
        "## References\n",
        "1. [Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, Wenwu Ou. 2019. arXiv.](https://arxiv.org/abs/1905.06874)\n",
        "2. [D-Roberts (2021) Transformer based recommendation systems [Source code].](https://github.com/D-Roberts/transformer-recommender)\n",
        "3. [shenweichen (2021) Easy-to-use,Modular and Extendible package of deep-learning based CTR models [Source code]](https://github.com/shenweichen/DeepCTR)\n",
        "4. [jiwidi (2021) Behavior-Sequence-Transformer-Pytorch [Source code].](https://github.com/jiwidi/Behavior-Sequence-Transformer-Pytorch)\n",
        "5. [nihalsangeeth (2021) Behaviour Sequence Transformers [Source code].](https://github.com/nihalsangeeth/behaviour-seq-transformer)\n",
        "6. [Khalid Salama (2020) A Transformer-based recommendation system [Source code].](https://keras.io/examples/structured_data/movielens_recommendations_transformers)\n",
        "7. [BST DeepCTR library](https://deepctr-doc.readthedocs.io/en/latest/deepctr.models.sequence.bst.html)\n",
        "8. [https://github.com/PaddlePaddle/PaddleRec/tree/release/1.8.5/models/rank/BST/](https://github.com/PaddlePaddle/PaddleRec/tree/release/1.8.5/models/rank/BST/)"
      ]
    }
  ]
}