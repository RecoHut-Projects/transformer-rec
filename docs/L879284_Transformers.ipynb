{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7e2867-1363-4c3d-aaa4-abbb8238c06d",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "The old, obsolete, 1980 architecture of Recurrent Neural Networks(RNNs) including the LSTMs were simply not producing good results anymore. In less than two years, transformer models wiped RNNs off the map and even outperformed human baselines for many tasks.\n",
    "\n",
    "Here‚Äôs what the transformer block looks like in PyTorch:\n",
    "\n",
    "```python\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, k, heads):\n",
    "    super().__init__()\n",
    "\n",
    "    self.attention = SelfAttention(k, heads=heads)\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(k)\n",
    "    self.norm2 = nn.LayerNorm(k)\n",
    "\n",
    "    self.ff = nn.Sequential(\n",
    "      nn.Linear(k, 4 * k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4 * k, k))\n",
    "\n",
    "  def forward(self, x):\n",
    "    attended = self.attention(x)\n",
    "    x = self.norm1(attended + x)\n",
    "    \n",
    "    fedforward = self.ff(x)\n",
    "    return self.norm2(fedforward + x)\n",
    "```\n",
    "\n",
    "Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.\n",
    "\n",
    "That is, the block applies, in sequence: a self attention layer, layer normalization, a feed forward layer (a single MLP applied independently to each vector), and another layer normalization. Residual connections are added around both, before the normalization. The order of the various components is not set in stone; the important thing is to combine self-attention with a local feedforward, and to add normalization and residual connections.\n",
    "\n",
    "<p><center><img src='_images/L879284_1.png'></center></p>\n",
    "\n",
    "## The transformer block\n",
    "\n",
    "1. Multi-head attention\n",
    "2. Scaling the dot product\n",
    "3. Queries, keys and values\n",
    "\n",
    "The actual self-attention used in modern transformers relies on three additional tricks. \n",
    "\n",
    "### Additional tricks\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assume we have some tensor x with size (b, t, k)\n",
    "x = ...\n",
    "\n",
    "raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "# - torch.bmm is a batched matrix multiplication. It \n",
    "#   applies matrix multiplication over batches of \n",
    "#   matrices.\n",
    "\n",
    "weights = F.softmax(raw_weights, dim=2)\n",
    "\n",
    "y = torch.bmm(weights, x)\n",
    "```\n",
    "\n",
    "We‚Äôll represent the input, a sequence of t vectors of dimension k as a t by k matrix ùêó. Including a minibatch dimension b, gives us an input tensor of size (b,t,k). The set of all raw dot products w‚Ä≤ij forms a matrix, which we can compute simply by multiplying ùêó by its transpose. Then, to turn the raw weights w‚Ä≤ij into positive values that sum to one, we apply a row-wise softmax. Finally, to compute the output sequence, we just multiply the weight matrix by ùêó. This results in a batch of output matrices ùêò of size (b, t, k) whose rows are weighted sums over the rows of ùêó. That‚Äôs all. Two matrix multiplications and one softmax gives us a basic self-attention.\n",
    "\n",
    "### In Pytorch: basic self-attention\n",
    "\n",
    "Let's understand with RecSys analogy. The tokens are both users and items. In movie recommenders e.g., to know a user's interest in different movies, we take a dot product of his embedding vector with movies' embedding vectors. Here in self-attention, we take a dot of the given token with all other tokens to know how much the given token is connected to other tokens.\n",
    "\n",
    "In effect, there are five processes we need to understand to implement this model:\n",
    "\n",
    "- Embedding the inputs\n",
    "- The Positional Encodings\n",
    "- Creating Masks\n",
    "- The Multi-Head Attention layer\n",
    "- The Feed-Forward layer\n",
    "\n",
    "### Embedding\n",
    "\n",
    "Embedding is handled simply in PyTorch:\n",
    "\n",
    "```python\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "```\n",
    "\n",
    "When each word is fed into the network, this code will perform a look-up and retrieve its embedding vector. These vectors will then be learnt as a parameters by the model, adjusted with each iteration of gradient descent.\n",
    "\n",
    "### Giving our words context: The Positional Encodings\n",
    "\n",
    "In order for the model to make sense of a sentence, it needs to know two things about each word: what does the word mean? And what is its position in the sentence?\n",
    "\n",
    "The embedding vector for each word will learn the meaning, so now we need to input something that tells the network about the word‚Äôs position.\n",
    "\n",
    "*Vasmani et al*¬†answered this problem by using these functions to create a constant of position-specific values:\n",
    "\n",
    "$$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$$\n",
    "\n",
    "This constant is a 2d matrix.¬†$*pos*$¬†refers to the order in the sentence, and¬†*i*¬†$$refers to the position along the embedding vector dimension. Each value in the $pos/i$ matrix is then worked out using the equations above.\n",
    "\n",
    "<p><center><img src='_images/L879284_2.png'></center></p>\n",
    "\n",
    "The positional encoding matrix is a constant whose values are defined by the above equations. When added to the embedding matrix, each word embedding is altered in a way specific to its position.\n",
    "\n",
    "An intuitive way of coding our Positional Encoder looks like this:\n",
    "\n",
    "```python\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], \\\n",
    "        requires_grad=False).cuda()\n",
    "        return x\n",
    "```\n",
    "\n",
    "The above module lets us add the positional encoding to the embedding vector, providing information about structure to the model.\n",
    "\n",
    "### **Creating Our Masks**\n",
    "\n",
    "Masking plays an important role in the transformer. It serves two purposes:\n",
    "\n",
    "- In the encoder and decoder: To zero attention outputs wherever there is just padding in the input sentences.\n",
    "- In the decoder: To prevent the decoder ‚Äòpeaking‚Äô ahead at the rest of the translated sentence when predicting the next word.\n",
    "\n",
    "Creating the mask for the input is simple:\n",
    "\n",
    "```python\n",
    "batch = next(iter(train_iter))\n",
    "input_seq = batch.English.transpose(0,1)\n",
    "input_pad = EN_TEXT.vocab.stoi['<pad>']# creates mask with 0s wherever there is padding in the input\n",
    "input_msk = (input_seq != input_pad).unsqueeze(1)\n",
    "```\n",
    "\n",
    "For the target_seq we do the same, but then create an additional step:\n",
    "\n",
    "```python\n",
    "# create mask as beforetarget_seq = batch.French.transpose(0,1)\n",
    "target_pad = FR_TEXT.vocab.stoi['<pad>']\n",
    "target_msk = (target_seq != target_pad).unsqueeze(1)size = target_seq.size(1) # get seq_len for matrixnopeak_mask= np.triu(np.ones(1, size, size),\n",
    "k=1).astype('uint8')\n",
    "nopeak_mask = Variable(torch.from_numpy(nopeak_mask)== 0)target_msk = target_msk & nopeak_mask\n",
    "```\n",
    "\n",
    "### **Multi-Headed Attention**\n",
    "\n",
    "Once we have our embedded values (with positional encodings) and our masks, we can start building the layers of our model.\n",
    "\n",
    "Here is an overview of the multi-headed attention layer:\n",
    "\n",
    "<p><center><img src='_images/L879284_3.png'></center></p>\n",
    "\n",
    "Multi-headed attention layer, each input is split into multiple heads which allows the network to simultaneously attend to different subsections of each embedding.\n",
    "\n",
    "A scaled dot-product attention mechanism is very similar to a self-attention (dot-product) mechanism except it uses a scaling factor. The multi-head part, on the other hand, ensures the model is capable of looking at various aspects of input at all levels. Transformer models attend to encoder annotations and the hidden values from past layers. The architecture of the Transformer model does not have a recurrent step-by-step flow; instead, it uses positional encoding in order to have information about the position of each token in the input sequence. The concatenated values of the embeddings (randomly initialized) and the fixed values of positional encoding are the input fed into the layers in the first encoder part and are propagated through the architecture.\n",
    "\n",
    "In the case of the Encoder,¬†*V, K*¬†and¬†*G*¬†will simply be identical copies of the embedding vector (plus positional encoding). They will have the dimensions Batch_size * seq_len * d_model.\n",
    "\n",
    "In multi-head attention we split the embedding vector into¬†*N*¬†heads, so they will then have the dimensions $batch\\_size * N * seq\\_len * (d\\_model / N).$\n",
    "\n",
    "This final dimension $(d\\_model / N )$ we will refer to as $d\\_k$.\n",
    "\n",
    "Let‚Äôs see the code for the decoder module:\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\t\t\t\t# calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "```\n",
    "\n",
    "### Calculating Attention\n",
    "\n",
    "<p><center><img src='_images/L879284_4.png'></center></p>\n",
    "\n",
    "$$Attention(Q,K,V) = \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}$$\n",
    "\n",
    "Initially we must multiply Q by the transpose of K. This is then ‚Äòscaled‚Äô by dividing the output by the square root of $d\\_k$. Before we perform Softmax, we apply our mask and hence reduce values where the input is padding (or in the decoder, also where the input is ahead of the current word). Finally, the last step is doing a dot product between the result so far and V.\n",
    "\n",
    "Here is the code for the attention function:\n",
    "\n",
    "```python\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "\n",
    "\t\tif mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "\t\tscores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "```\n",
    "\n",
    "In PyTorch, it looks like this:\n",
    "\n",
    "```python\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as f\n",
    "\n",
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)\n",
    "```\n",
    "\n",
    "Note that MatMul operations are translated to `torch.bmm` in PyTorch. That‚Äôs because Q, K, and V (query, key, and value arrays) are batches of matrices, each with shape `(batch_size, sequence_length, num_features)`. Batch matrix multiplication is only performed over the last two dimensions.\n",
    "\n",
    "The attention head will then become:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_k)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))\n",
    "```\n",
    "\n",
    "Now, it‚Äôs very easy to build the multi-head attention layer. Just combine `num_heads` different attention heads and a Linear layer for the output.\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )\n",
    "```\n",
    "\n",
    "Let‚Äôs pause again to examine what‚Äôs going on in the `MultiHeadAttention` layer. Each attention head computes its own query, key, and value arrays, and then applies scaled dot-product attention. Conceptually, this means each head can attend to a different part of the input sequence, independent of the others. Increasing the number of attention heads allows us to ‚Äúpay attention‚Äù to more parts of the sequence at once, which makes the model more powerful.\n",
    "\n",
    "### **The Feed-Forward Network**\n",
    "\n",
    "This layer just consists of two linear operations, with a relu and dropout operation in between them.\n",
    "\n",
    "```python\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "The feed-forward layer simply deepens our network, employing linear layers to analyze patterns in the attention layers output.\n",
    "\n",
    "### **One Last Thing : Normalization**\n",
    "\n",
    "Normalisation is highly important in deep neural networks. It prevents the range of values in the layers changing too much, meaning the model trains faster and has better ability to generalise.\n",
    "\n",
    "<p><center><img src='_images/L879284_5.png'></center></p>\n",
    "\n",
    "We will be normalizing our results between each layer in the encoder/decoder, so before building our model let‚Äôs define that function:\n",
    "\n",
    "```python\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "```\n",
    "\n",
    "### Putting it all together!\n",
    "\n",
    "Let‚Äôs have another look at the over-all architecture and start building:\n",
    "\n",
    "<p><center><img src='_images/L879284_6.png'></center></p>\n",
    "\n",
    "**One last Variable:**¬†If you look at the diagram closely you can see a ‚ÄòNx‚Äô next to the encoder and decoder architectures. In reality, the encoder and decoder in the diagram above represent one layer of an encoder and one of the decoder. N is the variable for the number of layers there will be. Eg. if N=6, the data goes through six encoder layers (with the architecture seen above), then these outputs are passed to the decoder which also consists of six repeating decoder layers.\n",
    "\n",
    "We will now build EncoderLayer and DecoderLayer modules with the architecture shown in the model above. Then when we build the encoder and decoder we can define how many of these layers to have.\n",
    "\n",
    "```python\n",
    "# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model).cuda()\n",
    "\n",
    "\t\tdef forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "\t\t# We can then build a convenient cloning function that can generate multiple layers:\n",
    "\t\tdef get_clones(module, N):\n",
    "\t\t    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "```\n",
    "\n",
    "We‚Äôre now ready to build the encoder and decoder:\n",
    "\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "```\n",
    "\n",
    "And finally‚Ä¶ The transformer!\n",
    "\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "# we don't perform softmax on the output as this will be handled \n",
    "# automatically by our loss function\n",
    "```\n",
    "\n",
    "## References\n",
    "1. [Transformers from scratch](http://peterbloem.nl/blog/transformers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
