{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "C001344_NARM.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e45ba068-ec1c-43e0-b0dd-e78a6e75fa81"
      },
      "source": [
        "# BERT4Rec"
      ],
      "id": "e45ba068-ec1c-43e0-b0dd-e78a6e75fa81"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE4D17G7xtK7"
      },
      "source": [
        "In BERT4Rec, authors used Cloze task technique (also known as “Masked Language Model) to train the bi-directional model. In this, we randomly mask some items (i.e., replace them with a special token [mask]) in the input sequences, and then predict the ids of those masked items based on their surrounding context. \n",
        "\n",
        "However, a downside of the Cloze task is that it is not consistent with the final task (i.e., sequential recommendation). To fix this, during the test, we append the special token “[mask]” at the end of the input sequence to indicate the item that we need to predict, and then make recommendations base on its final hidden vector.\n",
        "\n",
        "Attention mechanisms have become an integral part of sequence modeling in a variety of tasks, allowing capturing the dependencies between representation pairs without regard to their distance in the sequences. Specifically, multi-head attention first linearly projects H^l into h subspaces, with different, learnable linear projections, and then apply h attention functions in parallel to produce the output representations which are concatenated and once again projected. The temperature \\sqrt{d/h} is introduced to produce a softer attention distribution for avoiding extremely small gradients. The self-attention sub-layer is mainly based on linear projections. To endow the model with nonlinearity and interactions between different dimensions, we apply a Position-wise Feed-Forward Network to the outputs of the self-attention sub-layer, separately and identically at each position. It consists of two affine transformations with a Gaussian Error Linear Unit (GELU) activation in between.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "<p><center><img src='https://github.com/sparsh-ai/transformer-rec/blob/main/docs/_images/C759066_1.png?raw=1'>Differences in sequential recommendation model architectures. BERT4Rec learns a bidirectional model via Cloze task, while SASRec and RNN based methods are all left-to-right unidirectional model which predict next item sequentially.</center></p>"
      ],
      "id": "YE4D17G7xtK7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33a5eff8-6aff-4ee4-98bf-ebd0abadfda5"
      },
      "source": [
        "## References\n",
        "1. [Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. arXiv.](https://arxiv.org/abs/1904.06690)\n",
        "2. [FeiSun (2019) BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer [Source code].](https://github.com/FeiSun/BERT4Rec)\n",
        "3. [tunghia1890 (2021) BERT4Rec runable with Tensorflow 2.x [Source code].](https://github.com/tunghia1890/BERT4Rec_TF2x)\n",
        "4. [jaywonchung (2020) Pytorch implementation of BERT4Rec and Netflix VAE [Source code].](https://github.com/jaywonchung/BERT4Rec-VAE-Pytorch)\n",
        "5. [Youness Mansar (2021) Build Your Own Movie Recommender System Using BERT4Rec [Blog post]](https://towardsdatascience.com/build-your-own-movie-recommender-system-using-bert4rec-92e4e34938c5)\n",
        "6. [CVxTz (2021) recommender_transformer [Source code].](https://github.com/CVxTz/recommender_transformer)\n",
        "7. [https://recbole.io/docs/recbole/recbole.model.sequential_recommender.bert4rec.html](https://recbole.io/docs/recbole/recbole.model.sequential_recommender.bert4rec.html)"
      ],
      "id": "33a5eff8-6aff-4ee4-98bf-ebd0abadfda5"
    }
  ]
}