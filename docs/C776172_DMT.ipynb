{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C776172 | DMT: Deep Multifaceted Transformers",
      "provenance": [],
      "authorship_tag": "ABX9TyNC622bN4/cUAHbbtHgUzr8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz5dOYwP6cxk"
      },
      "source": [
        "# DMT\n",
        "\n",
        "> Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "568D8t6r6iAT"
      },
      "source": [
        "Recommender Systems have been playing essential roles in ecommerce portals. Existing recommendation algorithms usually learn the ranking scores of items by optimizing a single task (e.g., Click-through rate prediction) based on users’ historical click sequences, but they generally pay few attention to simultaneously modeling users’ multiple types of behaviors or jointly optimize multiple objectives (e.g., both Click-through rate and Conversion rate), which are both vital for e-commerce sites. In DMT (Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems), authors argued that it is crucial to formulate users’ different interests based on multiple types of behaviors and perform multi-task learning for significant improvement in multiple objectives simultaneously. DMT models users’ multiple types of behavior sequences simultaneously with multiple Transformers. It utilizes Multi-gate Mixture-of-Experts to optimize multiple objectives. Besides, it exploits unbiased learning to reduce the selection bias in the training data. Experiments on JD real production dataset demonstrate the effectiveness of DMT, which significantly outperforms state-of-the-art methods. DMT has been successfully deployed to serve the main traffic in the commercial Recommender System in [JD.com](http://jd.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMVEMhRz7BNP"
      },
      "source": [
        "## Architecture\n",
        "\n",
        "It utilizes Deep Multifaceted Transformers (bottom), which is consisted of multiple Deep Interest Transformers (right), to extract users’ multifaceted interests from their diverse behavior sequences, exploits Multi-gate Mixture-of-Experts (MMoE) (top) to simultaneously optimize multiple objectives, and uses a Bias Deep Neural Network (left) to reduce the bias in training data.\n",
        "\n",
        "<figure><p><center><img src='_images/C776172_1.png'><figcaption>DMT architecture</figcaption></figure></p></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSL6jtA7IXG"
      },
      "source": [
        "### Performance\n",
        "\n",
        "<p><center><img src='_images/C776172_2.png'></p></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgJM-yFt7wub"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [Paper on ResearchGate](https://www.researchgate.net/profile/Yulong-Gu-5/publication/344752297_Deep_Multifaceted_Transformers_for_Multi-objective_Rank-ing_in_Large-Scale_E-commerce_Recommender_Systems/links/5f8dbf1f299bf1b53e32af1c/Deep-Multifaceted-Transformers-for-Multi-objective-Rank-ing-in-Large-Scale-E-commerce-Recommender-Systems.pdf)\n",
        "2. [Git Repo](https://github.com/guyulongcs/CIKM2020_DMT)"
      ]
    }
  ]
}