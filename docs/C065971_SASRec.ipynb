{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "C065971_SASRec.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e45ba068-ec1c-43e0-b0dd-e78a6e75fa81"
      },
      "source": [
        "# SASRec"
      ],
      "id": "e45ba068-ec1c-43e0-b0dd-e78a6e75fa81"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33a5eff8-6aff-4ee4-98bf-ebd0abadfda5"
      },
      "source": [
        "Self-attention based sequential model (SASRec) captures the long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC). At each time step, SASRec seeks to identify which items are ‘relevant’ from a user’s action history, and use them to predict the next item. Extensive empirical studies show that this method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models."
      ],
      "id": "33a5eff8-6aff-4ee4-98bf-ebd0abadfda5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwmteqc4vekJ"
      },
      "source": [
        "## Architecture"
      ],
      "id": "fwmteqc4vekJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK06-TfjuewD"
      },
      "source": [
        "<p><center><img src='https://github.com/sparsh-ai/transformer-rec/blob/main/docs/_images/C065971_1.png?raw=1'>A simplified diagram showing the training process of SASRec. At each time step, the model considers all previous items, and uses attention to ‘focus on’ items relevant to the next action.</center></p>"
      ],
      "id": "DK06-TfjuewD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6YUpsnkvbxn"
      },
      "source": [
        "## Performance"
      ],
      "id": "y6YUpsnkvbxn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek_QKD2ZvigO"
      },
      "source": [
        "<p><center><img src='https://github.com/sparsh-ai/transformer-rec/blob/main/docs/_images/C065971_2.png?raw=1'></center></p>"
      ],
      "id": "ek_QKD2ZvigO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hz58Q9FvqXn"
      },
      "source": [
        "### Training efficiency on ML-1m"
      ],
      "id": "3Hz58Q9FvqXn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJwUzB_Wv2_N"
      },
      "source": [
        "<p><center><img src='https://github.com/sparsh-ai/transformer-rec/blob/main/docs/_images/C065971_3.png?raw=1'></center></p>"
      ],
      "id": "wJwUzB_Wv2_N"
    }
  ]
}